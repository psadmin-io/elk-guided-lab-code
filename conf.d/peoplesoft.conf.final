input {
  beats {
    type => beats
    port => 5044
  }
  file {
    type => "access_log"
    path => ["/tmp/elk-guided-lab-code/logs/webserv/psftdb/servers/PIA/logs/PIA_access.log*"]
    mode => "read"
    start_position => "beginning"
    file_completed_action => "log"
    file_completed_log_path => "/tmp/files.log"
  }
  file {
    type => "servlet_log"
    path => ["/tmp/elk-guided-lab-code/logs/webserv/psftdb/servers/PIA/logs/PIA_servlets*.log.*"]
    mode => "read"
    start_position => "beginning"
    file_completed_action => "log"
    file_completed_log_path => "/tmp/files.log"
  }
  file {
    type => "weblogic_log"
    path => ["/tmp/elk-guided-lab-code/logs/webserv/psftdb/servers/PIA/logs/PIA_stdout.log*"]
    mode => "read"
    start_position => "beginning"
    file_completed_action => "log"
    file_completed_log_path => "/tmp/files.log"
  }
  file {
    type => "appsrv_log"
    path => ["/tmp/elk-guided-lab-code/logs/appserv/psftdb/LOGS/APPSRV_*.LOG"]
    mode => "read"
    start_position => "beginning"
    file_completed_action => "log"
    file_completed_log_path => "/tmp/files.log"
  }
}

filter {

  # Drop empty log lines
  if [message] == "" {
    drop { }
  }

  # Web Server Access Logs
  if [type] == "access_log" {
    # drop comment lines
    if ([message] =~ /^#/) {
      drop{}
    }

    # WL Extended Log Format
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => { "message" => "%{WL_IO_EXTENDED}"}
      add_field => { "log" => "Access Log" }
      add_field => { "server type" => "webserv" }
      remove_tag => ["_grokparsefailure"]
    }

    if [tag] == "_grokparsefailure" {
      grok{
        match => { "message" => "%{COMMONAPACHELOG}"}
        add_field => { "log" => "Access Log" }
        add_field => { "server type" => "webserv" }
        remove_tag => ["_grokparsefailure"]
      }
    }

    if [request] {
      grok {
        patterns_dir => "/etc/logstash/conf.d/patterns"
        match => {"request" => "%{PS_URI_REQUEST}"}
        add_tag => ["PIA_reqeust"]
        remove_tag => ["_grokparsefailure"]
      }
      mutate {
        copy => {"request" => "log_message"}
      }
    }

    mutate {
      convert => {"duration" => "float"}
      convert => {"bytes" => "integer"}
    }

    kv {
      source => "request"
      target => "parameters"
      field_split => "&?"
      include_keys => ["SEARCH_GROUP", "PAGE"]
    }

    # Fix for tab-delimted time field... build a new field in a friendly format
    mutate {
      add_field => {
        "datetime" => "%{year}-%{month}-%{day} %{time}"
      }
    }

    date {
      match => ["datetime", "yyyy-MM-dd HH:mm:ss"]
      # remove_field => [ "datetime" ]
    }
  }
  if [type] == "weblogic_log" {
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => { "message" => "%{WLS_SRV_LOG}"}
      remove_tag => ["_grokparsefailure"]
    }

    mutate {
      add_field => {
        "datetime" => "%{year}-%{month}-%{day} %{time}"
      }
    }

    date {
      match => ["datetime", "yyyy-MMM-dd HH:mm:ss,SSS", "yyyy-MMM-d HH:mm:ss,SSS", "yyyy-MMM-dd H:mm:ss,SSS", "yyyy-MMM-d H:mm:ss,SSS"]
      # target => "@timestamp"
      # remove_field => ["datetime"]
    }

    if [threadid] == "weblogic.GCMonitor" {
      grok {
        patterns_dir => "/etc/logstash/conf.d/patterns"
        match => {"log_message" => "%{WLS_GC_MESSAGE}"}
        add_tag => ["jvm_heap"]
      }
    }

  }
  if [type] == "servlet_log" {
    # drop comment lines
    if ([message] =~ /^#/) {
      drop{}
    }

    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => { "message" => "%{SERVLET_LOG}"}
      add_field => { "log" => "Servlet Log" }
      add_field => { "server type" => "webserv" }
      remove_tag => ["_grokparsefailure"]
    }

    date {
      match => ["servlet_timestamp", "ISO8601"]
      # target => "@timestamp"
      # remove_field => ["servelet_timestamp"]
    }
  }

  #App Server APPSRV Logs
  if [type] == "appsrv_log" {
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => { "message" => "%{APPSRV_LOG}"}
      add_field => { "log" => "App Server Log" }
      add_field => { "server type" => "appserv" }
      remove_tag => ["_grokparsefailure"]
    }

    if [log_message] =~ "Authentication" {
      grok {
	      patterns_dir => "/etc/logstash/conf.d/patterns"
        match => { "log_message" => "%{LOGIN_DATA}"}
        add_tag => ["login"]
      }
    }

    if [log_message] =~ "SQL" {
      grok {
        patterns_dir => "/etc/logstash/conf.d/patterns"
        match => { "log_message" => "%{BAD_SQL}"}
        add_tag => ["sql_error"]
      }
    }

    if [log_message] =~ "Executing component" {
      grok {
        patterns_dir => "/etc/logstash/conf.d/patterns"
        match => { "log_message" => "%{EXEC_COMP}" }
        add_tag => ["comp_exec"]
      }

      # Create new entry that combines component name with durations
      aggregate {
        task_id => "%{vm}%{pid}%{service_request}"
        code => "map['componentName'] = event.get('componentName')"
        map_action => "create"
      }
    }

    if [log_message] =~ "elapsed time=" {
      grok {
        patterns_dir => "/etc/logstash/conf.d/patterns"
        match => { "log_message" => "%{SERVICE_DUR}" }
        add_tag => ["comp_duration"]
      }

      aggregate {
        task_id => "%{vm}%{pid}%{service_request}"
        code => "event.set('componentName', map['componentName'])"
        map_action => "update"
        end_of_task => true
        timeout => 600
        push_map_as_event_on_timeout => true
        add_tag => ["duration"]
      }
    }

    if [message] =~ "PSPAL" {
      grok {
        patterns_dir => "/etc/logstash/conf.d/patterns"
        match => { "message" => "%{CRASH}"}
        remove_tag => ["_grokparsefailure"]
        add_tag => ["crash"]
      }
    }

    date {
      match => ["app_timestamp", "MM/dd/YY HH:mm:ss", "MM/dd/YY H:mm:ss", "M/dd/YY HH:mm:ss", "M/dd/YY H:mm:ss",  "ISO8601"]
      # target => "@timestamp"
      # remove_field => [ "app_timestamp" ]
    }

    mutate {
      convert => {"duration" => "float"}
    }

  }

  if [message] =~ "is not started" {
    drop {}
  }
  if [type] == "appsrv_queue_log" {
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => { "message" => "%{APP_QUEUE}"}
      add_field => { "log" => "App Server Queue" }
      add_field => { "server_type" => "appserver"}
    }

    mutate {
      convert => {"active_processes" => "integer"}
      convert => {"queued_requests" => "integer"}
    }

    date {
      match => ["app_timestamp", "MM/dd/YY HH:mm:ss", "MM/dd/YY H:mm:ss", "M/dd/YY HH:mm:ss", "M/dd/YY H:mm:ss",  "ISO8601"]
      remove_field => [ "app_timestamp" ]
    }

  }

  if [type] == "appsrv_request_log" {

    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => { "message" => "%{APP_REQUEST}"}
      add_field => { "log" => "App Server Request" }
      add_field => { "server_type" => "appserver"}
    }

    mutate {
      convert => {"id" => "integer"}
      convert => {"requests_done" => "integer"}
      convert => {"load_done" => "integer"}
    }

    date {
      match => ["app_timestamp", "MM/dd/YY HH:mm:ss", "MM/dd/YY H:mm:ss", "M/dd/YY HH:mm:ss", "M/dd/YY H:mm:ss",  "ISO8601"]
      remove_field => [ "app_timestamp" ]
    }

  }

  if [useragent] {
    useragent {
      source=> "useragent"
      target => "agent"
    }
  }

  if [clientip] {
    geoip {
      source  => "clientip"
      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}" ]
    }
    mutate {
      convert => [ "[geoip][coordinates]", "float"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    manage_template => true
    template => "/etc/logstash/peoplesoft.template.json"
    index => "filebeat-%{+YYYY.MM.dd}"
    user => "elastic"
    password => "6GgW7voSgCMddYi0tUGs"
  }
  stdout {
    codec => "json"
  }
}
